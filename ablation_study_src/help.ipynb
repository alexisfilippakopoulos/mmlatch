{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/envs/mmlatch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stored_data(stored_folder):\n",
    "    audio_masks, text_masks, visual_masks, preds, targets = [], [], [], [], []\n",
    "    for i in range(1, 23):\n",
    "        visual_masks.append(torch.load(f\"{stored_folder}/masks/mask_visual_batch_{i}.pt\").cpu().numpy())\n",
    "        audio_masks.append(torch.load(f\"{stored_folder}/masks/mask_audio_batch_{i}.pt\").cpu().numpy())\n",
    "        text_masks.append(torch.load(f\"{stored_folder}/masks/mask_text_batch_{i}.pt\").cpu().numpy())\n",
    "        preds.append(torch.load(f\"{stored_folder}/preds/batch_{i}.pt\").cpu().numpy())\n",
    "        targets.append(torch.load(f\"{stored_folder}/labels/batch_{i}.pt\").cpu().numpy())\n",
    "\n",
    "    return audio_masks, text_masks, visual_masks, preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_samples(mask_list):\n",
    "    all_elements = []\n",
    "    for batch in mask_list:\n",
    "        unique_elem_list = [batch[i] for i in range(batch.shape[0])]\n",
    "        all_elements += unique_elem_list\n",
    "    return unique_elem_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22 22 22 22\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "audio_masks, text_masks, visual_masks, preds, targets = read_stored_data(stored_folder=\"../ablation_study/base\")\n",
    "print(len(audio_masks), len(visual_masks), len(text_masks), len(preds), len(targets))\n",
    "for i in range(len(audio_masks)):\n",
    "    print(audio_masks[i].shape[0] == text_masks[i].shape[0] == visual_masks[i].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 42, 74)\n",
      "(32, 68, 74)\n",
      "(32, 64, 74)\n",
      "(32, 41, 74)\n",
      "(32, 48, 74)\n",
      "(32, 122, 74)\n",
      "(32, 56, 74)\n",
      "(32, 40, 74)\n",
      "(32, 26, 74)\n",
      "(32, 43, 74)\n",
      "(32, 37, 74)\n",
      "(32, 48, 74)\n",
      "(32, 87, 74)\n",
      "(32, 61, 74)\n",
      "(32, 69, 74)\n",
      "(32, 95, 74)\n",
      "(32, 42, 74)\n",
      "(32, 33, 74)\n",
      "(32, 92, 74)\n",
      "(32, 51, 74)\n",
      "(32, 38, 74)\n",
      "(14, 38, 74)\n"
     ]
    }
   ],
   "source": [
    "for masks in audio_masks:\n",
    "    print(masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_ordered_samples(batches):\n",
    "    unique_samples = []\n",
    "    for batch in batches:\n",
    "        for sample in batch:\n",
    "            unique_samples.append(sample)\n",
    "\n",
    "    return unique_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "686 686 686 686 686\n"
     ]
    }
   ],
   "source": [
    "audio_masks = get_unique_ordered_samples(audio_masks)\n",
    "text_masks = get_unique_ordered_samples(text_masks)\n",
    "visual_masks = get_unique_ordered_samples(visual_masks)\n",
    "preds = [item for arr in preds for item in arr]\n",
    "targets = [item for arr in targets for item in arr]\n",
    "print(len(audio_masks), len(visual_masks), len(text_masks), len(preds), len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42, 74), (42, 74))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_masks[0].shape, audio_masks[31].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stored_data(stored_folder):\n",
    "    audio_masks, text_masks, visual_masks, preds, targets = [], [], [], [], []\n",
    "    for i in range(1, 23):\n",
    "        visual_masks.append(torch.load(f\"{stored_folder}/masks/mask_visual_batch_{i}.pt\"))\n",
    "        audio_masks.append(torch.load(f\"{stored_folder}/masks/mask_audio_batch_{i}.pt\"))\n",
    "        text_masks.append(torch.load(f\"{stored_folder}/masks/mask_text_batch_{i}.pt\"))\n",
    "        preds.append(torch.load(f\"{stored_folder}/preds/batch_{i}.pt\"))\n",
    "        targets.append(torch.load(f\"{stored_folder}/labels/batch_{i}.pt\"))\n",
    "\n",
    "    return audio_masks, text_masks, visual_masks, preds, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 22 22 22 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in range(len(audio_masks)):\\n    print(audio_masks[i].shape[0] == text_masks[i].shape[0] == visual_masks[i].shape[0])'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_masks, text_masks, visual_masks, preds, targets = read_stored_data(stored_folder=\"../ablation_study/base\")\n",
    "print(len(audio_masks), len(visual_masks), len(text_masks), len(preds), len(targets))\n",
    "\"\"\"for i in range(len(audio_masks)):\n",
    "    print(audio_masks[i].shape[0] == text_masks[i].shape[0] == visual_masks[i].shape[0])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((686, 74), (686, 300), (686, 35), (686,), (686,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `masks` is a list of (batch_size, time, features) tensors, one per batch\n",
    "all_audio_masks, all_visual_masks, all_text_masks = [], [], []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch_audio_mask, batch_text_mask, batch_vis_mask, preds, labels in zip(audio_masks, text_masks, visual_masks, preds, targets):\n",
    "    # Collapse over time with mean (shape: batch_size Ã— feature_dim)\n",
    "    audio_time_reduced = batch_audio_mask.mean(dim=1)\n",
    "    text_time_reduced = batch_text_mask.mean(dim=1)\n",
    "    visual_time_reduced = batch_vis_mask.mean(dim=1)\n",
    "    \n",
    "    all_audio_masks.append(audio_time_reduced.cpu().numpy())\n",
    "    all_text_masks.append(text_time_reduced.cpu().numpy())\n",
    "    all_visual_masks.append(visual_time_reduced.cpu().numpy())\n",
    "    all_preds.append(preds.cpu().numpy())\n",
    "    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "# Concatenate across all batches\n",
    "all_audio_masks = np.concatenate(all_audio_masks, axis=0)   # (N, 74)\n",
    "all_text_masks = np.concatenate(all_text_masks, axis=0)   # (N, 74)\n",
    "all_visual_masks = np.concatenate(all_visual_masks, axis=0)   # (N, 74)\n",
    "all_preds = np.array([item for arr in all_preds for item in arr])\n",
    "all_labels = np.array([item for arr in all_labels for item in arr])\n",
    "#all_preds = np.concatenate(all_preds, axis=0)   # (N,)\n",
    "#all_labels = np.concatenate(all_labels, axis=0) # (N,)\n",
    "\n",
    "all_audio_masks.shape, all_text_masks.shape, all_visual_masks.shape, all_preds.shape, all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmlatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
